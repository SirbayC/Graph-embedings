{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:17:15.868533200Z",
     "start_time": "2024-01-20T22:17:15.489533800Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Articles Dataset\n",
    "\n",
    "We provide a dataset of medium articles which have to be tagged to corresponding topics (software-development, artificial intellignece, Ui/UX). Along with articles we have subscriptions lists. The articles may be related by common subscription lists. The goal is to exploit this naturally occuring network structure for classifying articles to topics. Hence, it is a 3-way node classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:17:15.979532400Z",
     "start_time": "2024-01-20T22:17:15.512543300Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(dir_name: str, file_name: str):\n",
    "    \"\"\"Read the medium articles with lists\n",
    "\n",
    "    Args:\n",
    "        dir_name (str): Root directory of the medium title files and lists.\n",
    "\n",
    "    Returns:\n",
    "        final_data: merged dataframes with articles and lists\n",
    "    \"\"\"\n",
    "\n",
    "    final_data = pd.read_csv(dir_name+\"/\"+file_name+\".csv\")\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:17:16.097534200Z",
     "start_time": "2024-01-20T22:17:15.527535400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 article  \\\n0      https://medium.com/@maniakacademy/code-demo-sh...   \n1      https://medium.com/towards-artificial-intellig...   \n2      https://medium.com/@sarafparam/transformers-fo...   \n3      https://medium.com/towards-data-science/review...   \n4      https://medium.com/towards-data-science/a-comp...   \n...                                                  ...   \n27713  https://medium.com/cometheartbeat/deep-learnin...   \n27714  https://medium.com/towardsdev/intro-to-object-...   \n27715  https://medium.com/towards-data-science/learn-...   \n27716  https://medium.com/berndruecker/moving-from-em...   \n27717  https://guillaume-weingertner.medium.com/5-ste...   \n\n                                                   title  \\\n0      Code/Demo Share: Palo Alto Firewall Network In...   \n1                  Clustering using Social Graph Network   \n2                           Transformers for Time-Series   \n3      Reviewing A/B Testing Course by Google on Udacity   \n4      A Comprehensive Hands-on Guide to Transfer Lea...   \n...                                                  ...   \n27713   Deep Learning Techniques you Should Know in 2022   \n27714  Intro to Object-Oriented Programming For Data ...   \n27715                   Learn Enough Docker to be Useful   \n27716    Moving from embedded to remote workflow engines   \n27717  5 Steps to Build Beautiful Bar Charts with Python   \n\n                                                subtitle  \\\n0      IP is broken as a unit of Control! IDENTITY as...   \n1      A Social Graph Network can be formed when ther...   \n2      Forecasting still remains to be dominated by S...   \n3      Read to find out how A/B tests are performed a...   \n4      Deep Learning on Steroids with the Power of Kn...   \n...                                                  ...   \n27713  Over the years, Deep Learning has really taken...   \n27714  Implement a simple Linear Regression with OOP ...   \n27715  Part 1: The Conceptual Landscape — Containers ...   \n27716  For a long time, we have advocated for an arch...   \n27717  How to use the full capabilities of Matplotlib...   \n\n                      author        date  \\\n0           Sebastian Maniak  2022-08-17   \n1      Naveed Ahmed Janvekar  2022-01-29   \n2                Param Saraf  2020-10-20   \n3          Suyash Maheshwari  2020-05-10   \n4       Dipanjan (DJ) Sarkar  2018-11-14   \n...                      ...         ...   \n27713       Nisha Arya Ahmed  2022-04-21   \n27714                 Bex T.  2021-04-12   \n27715              Jeff Hale  2019-01-09   \n27716           Bernd Rücker  2022-02-08   \n27717  Guillaume Weingertner  2023-01-23   \n\n                                                    list  \n0      https://medium.com/@zemmali1990/list/aws-49f68...  \n1      https://medium.com/@TomaszCieplak/list/graph-d...  \n2      https://medium.com/@sergiobonato/list/time-ser...  \n3      https://medium.com/@online.rajib/list/ml-c2cac...  \n4      https://medium.com/@farhanhanavi07/list/deep-l...  \n...                                                  ...  \n27713  https://medium.com/@vigguvenki/list/deep-learn...  \n27714  https://medium.com/@or.matalon2/list/oop-4aad5...  \n27715  https://medium.com/@vaibhavb2473/list/machine-...  \n27716  https://medium.com/@giamma80/list/java-3c31810...  \n27717  https://luistrigueiros.medium.com/list/python-...  \n\n[27718 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>title</th>\n      <th>subtitle</th>\n      <th>author</th>\n      <th>date</th>\n      <th>list</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://medium.com/@maniakacademy/code-demo-sh...</td>\n      <td>Code/Demo Share: Palo Alto Firewall Network In...</td>\n      <td>IP is broken as a unit of Control! IDENTITY as...</td>\n      <td>Sebastian Maniak</td>\n      <td>2022-08-17</td>\n      <td>https://medium.com/@zemmali1990/list/aws-49f68...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://medium.com/towards-artificial-intellig...</td>\n      <td>Clustering using Social Graph Network</td>\n      <td>A Social Graph Network can be formed when ther...</td>\n      <td>Naveed Ahmed Janvekar</td>\n      <td>2022-01-29</td>\n      <td>https://medium.com/@TomaszCieplak/list/graph-d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://medium.com/@sarafparam/transformers-fo...</td>\n      <td>Transformers for Time-Series</td>\n      <td>Forecasting still remains to be dominated by S...</td>\n      <td>Param Saraf</td>\n      <td>2020-10-20</td>\n      <td>https://medium.com/@sergiobonato/list/time-ser...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://medium.com/towards-data-science/review...</td>\n      <td>Reviewing A/B Testing Course by Google on Udacity</td>\n      <td>Read to find out how A/B tests are performed a...</td>\n      <td>Suyash Maheshwari</td>\n      <td>2020-05-10</td>\n      <td>https://medium.com/@online.rajib/list/ml-c2cac...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://medium.com/towards-data-science/a-comp...</td>\n      <td>A Comprehensive Hands-on Guide to Transfer Lea...</td>\n      <td>Deep Learning on Steroids with the Power of Kn...</td>\n      <td>Dipanjan (DJ) Sarkar</td>\n      <td>2018-11-14</td>\n      <td>https://medium.com/@farhanhanavi07/list/deep-l...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27713</th>\n      <td>https://medium.com/cometheartbeat/deep-learnin...</td>\n      <td>Deep Learning Techniques you Should Know in 2022</td>\n      <td>Over the years, Deep Learning has really taken...</td>\n      <td>Nisha Arya Ahmed</td>\n      <td>2022-04-21</td>\n      <td>https://medium.com/@vigguvenki/list/deep-learn...</td>\n    </tr>\n    <tr>\n      <th>27714</th>\n      <td>https://medium.com/towardsdev/intro-to-object-...</td>\n      <td>Intro to Object-Oriented Programming For Data ...</td>\n      <td>Implement a simple Linear Regression with OOP ...</td>\n      <td>Bex T.</td>\n      <td>2021-04-12</td>\n      <td>https://medium.com/@or.matalon2/list/oop-4aad5...</td>\n    </tr>\n    <tr>\n      <th>27715</th>\n      <td>https://medium.com/towards-data-science/learn-...</td>\n      <td>Learn Enough Docker to be Useful</td>\n      <td>Part 1: The Conceptual Landscape — Containers ...</td>\n      <td>Jeff Hale</td>\n      <td>2019-01-09</td>\n      <td>https://medium.com/@vaibhavb2473/list/machine-...</td>\n    </tr>\n    <tr>\n      <th>27716</th>\n      <td>https://medium.com/berndruecker/moving-from-em...</td>\n      <td>Moving from embedded to remote workflow engines</td>\n      <td>For a long time, we have advocated for an arch...</td>\n      <td>Bernd Rücker</td>\n      <td>2022-02-08</td>\n      <td>https://medium.com/@giamma80/list/java-3c31810...</td>\n    </tr>\n    <tr>\n      <th>27717</th>\n      <td>https://guillaume-weingertner.medium.com/5-ste...</td>\n      <td>5 Steps to Build Beautiful Bar Charts with Python</td>\n      <td>How to use the full capabilities of Matplotlib...</td>\n      <td>Guillaume Weingertner</td>\n      <td>2023-01-23</td>\n      <td>https://luistrigueiros.medium.com/list/python-...</td>\n    </tr>\n  </tbody>\n</table>\n<p>27718 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = pd.read_csv(\"data/pipeline_assignment_data/full_data_without_labels.csv\")\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:31:17.636857500Z",
     "start_time": "2024-01-20T22:31:17.363856900Z"
    }
   },
   "outputs": [],
   "source": [
    "train = read_data(\"data/pipeline_assignment_data\",\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:17:16.343533500Z",
     "start_time": "2024-01-20T22:17:16.149534800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      index                                            article  \\\n0      2291  https://medium.com/towards-data-science/how-to...   \n1      7292  https://medium.com/towards-data-science/surviv...   \n2      6768  https://medium.com/experience-stack/embrace-co...   \n3     15003  https://medium.com/towards-data-science/every-...   \n4     19782  https://medium.com/towards-data-science/macroe...   \n...     ...                                                ...   \n3945  14424  https://medium.com/towards-data-science/normal...   \n3946  18949  https://medium.com/@petruknisme/getting-starte...   \n3947   9305  https://medium.com/towards-data-science/how-to...   \n3948  24337  https://medium.com/towards-data-science/how-to...   \n3949   7662  https://medium.com/towards-data-science/a-prac...   \n\n                                                  title  \\\n0     How to Use the IBM Watson Tone Analyzer to Per...   \n1     Survival Analysis: Intuition & Implementation ...   \n2                           Embrace Complexity (Part 1)   \n3     Every Complex DataFrame Manipulation, Explaine...   \n4     Macroeconomic & Financial Factors and Ordinary...   \n...                                                 ...   \n3945  Normalization vs Standardization — Quantitativ...   \n3946   Getting Started with Covenant C2 for Red Teaming   \n3947  How to Create a Vector-Based Movie Recommendat...   \n3948                     How to Use Pandas for Big Data   \n3949  A Practical Guide to Build an Enterprise Knowl...   \n\n                                               subtitle  \\\n0     How to use the IBM Watson Artificial Intellige...   \n1     There is a statistical technique which can ans...   \n2     Why all organisations should build internal ne...   \n3     Melts, pivots, joins, explodes, & more — Panda...   \n4     Econometrics model using Arbitrage Pricing The...   \n...                                                 ...   \n3945  Stop using StandardScaler from Sklearn as a de...   \n3946  Command and Control is part of Red Teaming tac...   \n3947  Building a movie recommendation system using t...   \n3948  Run distributed workload with Pandas on Spark ...   \n3949  How to solve the practical problems when build...   \n\n                       author        date  \\\n0             Graham Harrison  2022-01-02   \n1               Anurag Pandey  2019-01-06   \n2                  Tony Seale  2022-02-04   \n3                    Andre Ye  2020-07-22   \n4                Sarit Maitra  2020-06-27   \n...                       ...         ...   \n3945              Shay Geller  2019-04-04   \n3946                      Aan  2021-11-21   \n3947  Michelangiolo Mazzeschi  2021-12-10   \n3948                Edwin Tan  2022-01-25   \n3949                 Xu LIANG  2019-10-23   \n\n                                                   list  \\\n0     https://medium.com/@4ndres.gaviria/list/nlp-to...   \n1     https://medium.com/@jz5246/list/analytics-559c...   \n2     https://medium.com/@yasha.brener/list/data-man...   \n3     https://medium.com/@4ndres.gaviria/list/dataop...   \n4     https://medium.com/@halo9pan/list/quantitative...   \n...                                                 ...   \n3945  https://medium.com/@farhanhanavi07/list/applie...   \n3946  https://medium.com/@jimmy.winghang/list/mitre-...   \n3947  https://medium.com/@subhasis.jethy/list/recomm...   \n3948  https://medium.com/@jethro_torczon/list/big-da...   \n3949  https://medium.com/@adambouras1/list/semantic-...   \n\n                       labels  \n0     artificial-intelligence  \n1     artificial-intelligence  \n2        software-development  \n3     artificial-intelligence  \n4        software-development  \n...                       ...  \n3945  artificial-intelligence  \n3946     software-development  \n3947  artificial-intelligence  \n3948  artificial-intelligence  \n3949  artificial-intelligence  \n\n[3950 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>article</th>\n      <th>title</th>\n      <th>subtitle</th>\n      <th>author</th>\n      <th>date</th>\n      <th>list</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2291</td>\n      <td>https://medium.com/towards-data-science/how-to...</td>\n      <td>How to Use the IBM Watson Tone Analyzer to Per...</td>\n      <td>How to use the IBM Watson Artificial Intellige...</td>\n      <td>Graham Harrison</td>\n      <td>2022-01-02</td>\n      <td>https://medium.com/@4ndres.gaviria/list/nlp-to...</td>\n      <td>artificial-intelligence</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7292</td>\n      <td>https://medium.com/towards-data-science/surviv...</td>\n      <td>Survival Analysis: Intuition &amp; Implementation ...</td>\n      <td>There is a statistical technique which can ans...</td>\n      <td>Anurag Pandey</td>\n      <td>2019-01-06</td>\n      <td>https://medium.com/@jz5246/list/analytics-559c...</td>\n      <td>artificial-intelligence</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6768</td>\n      <td>https://medium.com/experience-stack/embrace-co...</td>\n      <td>Embrace Complexity (Part 1)</td>\n      <td>Why all organisations should build internal ne...</td>\n      <td>Tony Seale</td>\n      <td>2022-02-04</td>\n      <td>https://medium.com/@yasha.brener/list/data-man...</td>\n      <td>software-development</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15003</td>\n      <td>https://medium.com/towards-data-science/every-...</td>\n      <td>Every Complex DataFrame Manipulation, Explaine...</td>\n      <td>Melts, pivots, joins, explodes, &amp; more — Panda...</td>\n      <td>Andre Ye</td>\n      <td>2020-07-22</td>\n      <td>https://medium.com/@4ndres.gaviria/list/dataop...</td>\n      <td>artificial-intelligence</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19782</td>\n      <td>https://medium.com/towards-data-science/macroe...</td>\n      <td>Macroeconomic &amp; Financial Factors and Ordinary...</td>\n      <td>Econometrics model using Arbitrage Pricing The...</td>\n      <td>Sarit Maitra</td>\n      <td>2020-06-27</td>\n      <td>https://medium.com/@halo9pan/list/quantitative...</td>\n      <td>software-development</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3945</th>\n      <td>14424</td>\n      <td>https://medium.com/towards-data-science/normal...</td>\n      <td>Normalization vs Standardization — Quantitativ...</td>\n      <td>Stop using StandardScaler from Sklearn as a de...</td>\n      <td>Shay Geller</td>\n      <td>2019-04-04</td>\n      <td>https://medium.com/@farhanhanavi07/list/applie...</td>\n      <td>artificial-intelligence</td>\n    </tr>\n    <tr>\n      <th>3946</th>\n      <td>18949</td>\n      <td>https://medium.com/@petruknisme/getting-starte...</td>\n      <td>Getting Started with Covenant C2 for Red Teaming</td>\n      <td>Command and Control is part of Red Teaming tac...</td>\n      <td>Aan</td>\n      <td>2021-11-21</td>\n      <td>https://medium.com/@jimmy.winghang/list/mitre-...</td>\n      <td>software-development</td>\n    </tr>\n    <tr>\n      <th>3947</th>\n      <td>9305</td>\n      <td>https://medium.com/towards-data-science/how-to...</td>\n      <td>How to Create a Vector-Based Movie Recommendat...</td>\n      <td>Building a movie recommendation system using t...</td>\n      <td>Michelangiolo Mazzeschi</td>\n      <td>2021-12-10</td>\n      <td>https://medium.com/@subhasis.jethy/list/recomm...</td>\n      <td>artificial-intelligence</td>\n    </tr>\n    <tr>\n      <th>3948</th>\n      <td>24337</td>\n      <td>https://medium.com/towards-data-science/how-to...</td>\n      <td>How to Use Pandas for Big Data</td>\n      <td>Run distributed workload with Pandas on Spark ...</td>\n      <td>Edwin Tan</td>\n      <td>2022-01-25</td>\n      <td>https://medium.com/@jethro_torczon/list/big-da...</td>\n      <td>artificial-intelligence</td>\n    </tr>\n    <tr>\n      <th>3949</th>\n      <td>7662</td>\n      <td>https://medium.com/towards-data-science/a-prac...</td>\n      <td>A Practical Guide to Build an Enterprise Knowl...</td>\n      <td>How to solve the practical problems when build...</td>\n      <td>Xu LIANG</td>\n      <td>2019-10-23</td>\n      <td>https://medium.com/@adambouras1/list/semantic-...</td>\n      <td>artificial-intelligence</td>\n    </tr>\n  </tbody>\n</table>\n<p>3950 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = read_data(\"data/pipeline_assignment_data\",\"test\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T00:06:40.312288800Z",
     "start_time": "2024-01-21T00:06:40.281420900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "labels\nsoftware-development       11586\nartificial-intelligence    10646\nux                           150\nName: count, dtype: int64"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:17:16.405534100Z",
     "start_time": "2024-01-20T22:17:16.245534200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "labels\nsoftware-development       2022\nartificial-intelligence    1899\nux                           29\nName: count, dtype: int64"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:17:16.406533900Z",
     "start_time": "2024-01-20T22:17:16.260534200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0        With Kazam and ffmpeg on GNU/Linux systems — I...\n1        A tutorial on how to deploy SpaCy with AWS. — ...\n2        Are you a senior analyst growing towards a man...\n3        My Dart Google Summer of Code 2021 experience....\n4        Understand how to discover multicollinearity i...\n                               ...                        \n22377    How to design your own graph using TigerGraph ...\n22378    Automate the editing of explainer videos to cr...\n22379    Faster Python Code With Numba — The Speed Issu...\n22380    Julia is used for a lot of deeply technical ap...\n22381    For years now, most of us have heard the word ...\nName: subtitle, Length: 22382, dtype: object"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.subtitle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the separated \"train\" and \"test\" data also contain an index column. The value in that index corresponds to the index of the row that article appears in the full data.\n",
    "\n",
    "Note that some of the articles are repeated (when considering articles uniquely identified by their urls). Thus it might not be necessarily wise to use the index of an article itself as a unique identifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27718\n",
      "26660\n"
     ]
    }
   ],
   "source": [
    "article_links = final_data[\"article\"]\n",
    "print(len(article_links))\n",
    "print(len(set(article_links)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:41:11.362066400Z",
     "start_time": "2024-01-21T00:41:11.336066800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:17:16.427534100Z",
     "start_time": "2024-01-20T22:17:16.276534300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\"\"\"Generate encoding for labels using label encoder\"\"\"\n",
    "multilabel_binarizer = LabelEncoder()\n",
    "multilabel_binarizer.fit(train[\"labels\"]) \n",
    "\n",
    "Y = multilabel_binarizer.transform(train[\"labels\"]) \n",
    "texts = [x[0]+\" \" + x[1] for x in zip(train.title,train.subtitle)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the 3 labels have been converted into a numerical value:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The labels: ['artificial-intelligence', 'software-development', 'ux'] have been respectively assigned the values: {0, 1, 2}\n",
      "The label of the first article is: software-development and was assigned label index: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'The labels: {list(multilabel_binarizer.classes_)} have been respectively assigned the values: {set(Y)}')\n",
    "print(f'The label of the first article is: {train[\"labels\"][0]} and was assigned label index: {Y[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T22:44:54.918268100Z",
     "start_time": "2024-01-20T22:44:54.868269700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Word2Vec model prediction\n",
    "\n",
    "We will train the model on the corpus composed of a list of a list of the words per article title + subtitle pair."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code/Demo Share: Palo Alto Firewall Network Infrastructure Automation with Consul-Terraform-Sync IP is broken as a unit of Control! IDENTITY as a unit of control is the key to succeed in discovering, securing and automating your current workflows across any runtime and cloud. Let’s discuss how we can automate Palo Alto Network firewall dynamic address groups using Consul with Terraform. The…\n",
      "['Code/Demo', 'Share:', 'Palo', 'Alto', 'Firewall', 'Network', 'Infrastructure', 'Automation', 'with', 'Consul-Terraform-Sync', 'IP', 'is', 'broken', 'as', 'a', 'unit', 'of', 'Control!', 'IDENTITY', 'as', 'a', 'unit', 'of', 'control', 'is', 'the', 'key', 'to', 'succeed', 'in', 'discovering,', 'securing', 'and', 'automating', 'your', 'current', 'workflows', 'across', 'any', 'runtime', 'and', 'cloud.', 'Let’s', 'discuss', 'how', 'we', 'can', 'automate', 'Palo', 'Alto', 'Network', 'firewall', 'dynamic', 'address', 'groups', 'using', 'Consul', 'with', 'Terraform.', 'The…']\n",
      "We will train on 27718 article descriptions, with 1756693 words in total, with 101496 unique entries.\n"
     ]
    }
   ],
   "source": [
    "all_texts = [x[0]+ \" \" + x[1] for x in zip(final_data.title,final_data.subtitle)]\n",
    "print(all_texts[0])\n",
    "\n",
    "all_texts_split = [text.split(\" \") for text in all_texts]\n",
    "print(all_texts_split[0])\n",
    "\n",
    "unique_values_set = set()\n",
    "for sublist in all_texts_split:\n",
    "    for value in sublist:\n",
    "        unique_values_set.add(value)\n",
    "\n",
    "print(f'We will train on {len(all_texts_split)} article descriptions, with {sum([len(v) for v in all_texts_split])} words in total, with {len(unique_values_set)} unique entries.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:50:09.808413700Z",
     "start_time": "2024-01-20T23:50:09.088417200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:02.049364800Z",
     "start_time": "2024-01-20T22:17:16.358534400Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\"\"\"Train word2vec model on title + subtitles to establish a baseline without network structure\"\"\"\n",
    "word2vec_model = Word2Vec(all_texts_split, vector_size=128, window=10, epochs=30, sg=1, workers=4,min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sample usage of the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding for the word \"Spring\" is [ 0.12796558  0.81976956  0.76871574 ... -0.4356603   0.25822845\n",
      " -0.4001068 ], with shape: (128,)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=10)\n",
    "print(f'The embedding for the word \"Spring\" is {word2vec_model.wv[\"Spring\"]}, with shape: {word2vec_model.wv[\"Spring\"].shape}')\n",
    "\n",
    "# Running the model on an unseen word will raise an error!"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:24:15.349508500Z",
     "start_time": "2024-01-20T23:24:15.286963300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:38:19.214639Z",
     "start_time": "2024-01-20T23:38:16.009127800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape (22382, 128) corresponds to the 128 parameter long encoding of each article.\n",
      "This corresponds to Y array with (22382,) entries, containing the labels for each article.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "embeddings = []\n",
    "\"\"\"Infer word2vec embeddings for article titles and subtitles using trained word2vec model\"\"\"\n",
    "for text in texts:\n",
    "    # Each text is one article (title + subtitle)\n",
    "    embeddings.append(\n",
    "        np.mean(\n",
    "            [word2vec_model.wv[word] for word in text.split(\" \")] # At this point, we will have an array of size (n, 128) where n is the number of words in the title + subtitle\n",
    "            , axis=0 # We compress each column, thus we obtain an array of size (1,128) which represents the embedding of each article (text)\n",
    "        ) # We add the embedding to our list\n",
    "    )\n",
    "    \n",
    "X_word2vec = np.vstack(embeddings)\n",
    "print(f'The shape {X_word2vec.shape} corresponds to the 128 parameter long encoding of each article.')\n",
    "print(f'This corresponds to Y array with {Y.shape} entries, containing the labels for each article.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:52:12.035166700Z",
     "start_time": "2024-01-20T23:51:37.676100400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "SVC()",
      "text/html": "<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Train a SVM classifier on generated article embeddings\"\"\"\n",
    "svc = SVC()\n",
    "svc.fit(X_word2vec,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:53:32.690748300Z",
     "start_time": "2024-01-20T23:53:31.805676800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape (3950, 128) corresponds to the 128 parameter long encoding of each article in the test data.\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = []\n",
    "test_texts = [x[0]+ \" \" + x[1] for x in zip(test.title,test.subtitle)]\n",
    "\"\"\"Compute embeddings for test samples\"\"\"\n",
    "for text in test_texts:\n",
    "    test_embeddings.append(np.mean([word2vec_model.wv[word] for word in text.split(\" \")], axis=0))\n",
    "X_word2vec_test = np.vstack(test_embeddings)\n",
    "print(f'The shape {X_word2vec_test.shape} corresponds to the 128 parameter long encoding of each article in the test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:56:57.821422Z",
     "start_time": "2024-01-20T23:56:48.917389600Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Generate predictions using the SVM classifier for test articles\"\"\"\n",
    "predictions = svc.predict(X_word2vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:56:57.837388500Z",
     "start_time": "2024-01-20T23:56:57.824388800Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Encode reference labels\"\"\"\n",
    "Y_test = multilabel_binarizer.transform(test[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add this point we have a first attempt at predicting the label for articles in our test set, without using the list information at all, rather just analyzing the embeddings of each article's description. We can see that both the predicted and Y_test have the same number of elements (as a sanity check - we are testing on 3950 articles). \n",
    "\n",
    "However, we can already spot that we have not predicted any articles having the \"UX\" label. Granted, these were only 29 out of 3950, or 0.73%, with a similarly low percentage in the training data (0.67%)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3950,)\n",
      "{0, 1}\n",
      "(3950,)\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(set(predictions))\n",
    "print(Y_test.shape)\n",
    "print(set(Y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:59:49.893959800Z",
     "start_time": "2024-01-20T23:59:49.867924400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:46.388393200Z",
     "start_time": "2024-01-20T22:20:46.347395600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5515588133987772\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\"\"\"Compute Macro f1\"\"\"\n",
    "print(metrics.f1_score(Y_test, predictions,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T00:07:41.483327200Z",
     "start_time": "2024-01-21T00:07:41.405304100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83      1899\n",
      "           1       0.84      0.82      0.83      2022\n",
      "           2       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.82      3950\n",
      "   macro avg       0.55      0.55      0.55      3950\n",
      "weighted avg       0.82      0.82      0.82      3950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\CodingProjects\\Graph-embedings\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\CodingProjects\\Graph-embedings\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\CodingProjects\\Graph-embedings\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph embeddings - Pipeline Overview \n",
    "\n",
    "Our goal is to construct a graph from the given data by connecting nodes that at least share one common subscription list.\n",
    "\n",
    "This step is followed by a random walk to construct node embeddings.\n",
    "\n",
    "Then the node embeddings are employed for the task of topic classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming the graph\n",
    "\n",
    "Our goal is to contruct a graph from  given data by connecting nodes that atleast share one common subscription list. The networkx part has already been written for you. \n",
    "\n",
    "Your task here is to write the module to construct the edges and find isolated nodes and also analyze the resulting graph by reporting number of edges, number of nodes, number of isolated nodes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "from typing import List,Dict\n",
    "\n",
    "def get_nodes_url(data: pd.DataFrame) -> Dict:\n",
    "    \"\"\" Given the dataframe with articles and lists return the set of nodes\n",
    "        Args:\n",
    "        data (pd.DataFrame): The medium dataset\n",
    "    Returns:\n",
    "        nodes: dict maps a url to its main id\"\"\"\n",
    "    nodes = {}\n",
    "    url_to_node = {}\n",
    "    for index, row in data.iterrows():\n",
    "        if row[\"article\"] not in url_to_node:\n",
    "            nodes[index] = [row[\"article\"]]\n",
    "            url_to_node[row[\"article\"]] = index\n",
    "        else:\n",
    "            # add alternative id\n",
    "            nodes[url_to_node[row[\"article\"]]].append(index)\n",
    "    return url_to_node"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:49:35.161737300Z",
     "start_time": "2024-01-21T01:49:35.142710500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T01:51:52.984305800Z",
     "start_time": "2024-01-21T01:51:52.957305800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_edges(data: pd.DataFrame, nodes: Dict) -> List:\n",
    "    \"\"\" Given the dataframe with articles and lists return the set of edges\n",
    "        Args:\n",
    "        data (pd.DataFrame): The medium dataset\n",
    "        nodes: dict (nodeId: article title)\n",
    "    Returns:\n",
    "            edges (List[tuple]): List of edges\"\"\"\n",
    "    edges = []    \n",
    "    ## START\n",
    "    url_to_mainId = get_nodes_url(data)\n",
    "    lists = {}\n",
    "    for index, row in data.iterrows():\n",
    "        mainId = -1\n",
    "        if index in nodes.keys():\n",
    "            mainId = index\n",
    "        else:\n",
    "            mainId = url_to_mainId[row[\"article\"]]\n",
    "        if row[\"list\"] in lists.keys():\n",
    "            lists[row[\"list\"]].append(mainId)\n",
    "        else:\n",
    "            lists[row[\"list\"]] = [mainId]\n",
    "            \n",
    "    \n",
    "    for list_values in lists:\n",
    "        n = len(list_values)\n",
    "        for i in range(0, n):\n",
    "            for j in range(i+1, n):\n",
    "                edges.append((list_values[i], list_values[j]))\n",
    "    print(len(edges))\n",
    "    ##END\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T01:51:53.298740Z",
     "start_time": "2024-01-21T01:51:53.279740500Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nodes(data: pd.DataFrame) -> Dict:\n",
    "    \"\"\" Given the dataframe with articles and lists return the set of nodes\n",
    "        Args:\n",
    "        data (pd.DataFrame): The medium dataset\n",
    "    Returns:\n",
    "        nodes: dict maps a main id to its url (first element in the value list), then alternative ids (defined below)\"\"\"\n",
    "    nodes = {}\n",
    "    url_to_node = {}\n",
    "    for index, row in data.iterrows():\n",
    "        if row[\"article\"] not in url_to_node:\n",
    "            nodes[index] = [row[\"article\"]]\n",
    "            url_to_node[row[\"article\"]] = index\n",
    "        else:\n",
    "            # add alternative id\n",
    "            nodes[url_to_node[row[\"article\"]]].append(index)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265226622\n"
     ]
    }
   ],
   "source": [
    "nodes = get_nodes(final_data)\n",
    "edges = get_edges(final_data, nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:52:50.931662700Z",
     "start_time": "2024-01-21T01:51:54.066063400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def get_nodes_url(data: pd.DataFrame) -> Dict:\n",
    "    \"\"\" Given the dataframe with articles and lists return the set of nodes\n",
    "        Args:\n",
    "        data (pd.DataFrame): The medium dataset\n",
    "    Returns:\n",
    "        nodes: dict maps a main id to its url (first element in the value list), then alternative ids (defined below)\"\"\"\n",
    "    nodes = {}\n",
    "    url_to_node = {}\n",
    "    for index, row in data.iterrows():\n",
    "        if row[\"article\"] not in url_to_node:\n",
    "            nodes[index] = [row[\"article\"]]\n",
    "            url_to_node[row[\"article\"]] = index\n",
    "        else:\n",
    "            # add alternative id\n",
    "            nodes[url_to_node[row[\"article\"]]].append(index)\n",
    "    return nodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:38:54.076088800Z",
     "start_time": "2024-01-21T01:38:54.026988400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that, as per our earlier discussion, the article urls are not unique in the full data. Thus, we will modify the get_nodes to reflect this fact. We will use the row index of the first appearance of a url to be the main id. Then, in order to capture eventual repeating values, we will also store the other indexes (alternative ids) of the same article as a list in the returned dictionary. Notice that the urls are also unique in the returned dictionary, linked 1 to 1 to the main id.\n",
    "\n",
    "For example: 0 -> \\[\"url\", 1, 2] means that the article uniquely identified by the \"url\" has the main id 0 (row where it first appear), and alternative ids 1 and 2.\n",
    "\n",
    "Running our code, we can see that for 1058 unique articles, their url appeared more than once (in fact, twice) in the full data. For each of these cases, we have also stored the alternative id. Moreover, 1054 out of the 1058 articles also appear in our labeled data, so our (complex) pre-processing can be justified."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21689\n",
      "3921\n",
      "1058\n",
      "2\n",
      "(26685, ['https://medium.com/@maniakacademy/automating-application-delivery-with-consul-nia-part-1-f5-big-ip-b5e30138be5c', 26875])\n",
      "1054\n"
     ]
    }
   ],
   "source": [
    "nodes = get_nodes(final_data)\n",
    "\n",
    "train_url_set = set(train[\"article\"])\n",
    "train_test_set = set(test[\"article\"])\n",
    "\n",
    "count_alternative_ids = 0\n",
    "max_count = 0\n",
    "example_value = ()\n",
    "\n",
    "duplicated_count_labeled = 0\n",
    "\n",
    "for key, value in nodes.items():\n",
    "    if len(value) > 1:\n",
    "        if value[0] in train_url_set or value[0] in train_test_set:\n",
    "            duplicated_count_labeled+=1\n",
    "        example_value = (key, value)\n",
    "        count_alternative_ids+=1\n",
    "        max_count = max(max_count, len(value))\n",
    "print(count_alternative_ids)\n",
    "print(max_count)\n",
    "print(example_value)\n",
    "\n",
    "print(duplicated_count_labeled)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:36:09.266823900Z",
     "start_time": "2024-01-21T01:36:07.638315500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:46.531392800Z",
     "start_time": "2024-01-20T22:20:46.428392900Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def form_graph(data: pd.DataFrame) -> nx.Graph:\n",
    "    \"\"\"Forms graph from medium article dataset.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The medium dataset\n",
    "\n",
    "    Returns:\n",
    "        G (nx.Graph): The graph.\n",
    "\n",
    "       \"\"\"\n",
    "    texts = [x[0]+\" \" + x[1] for x in zip(data.title,data.subtitle)]\n",
    "    nodes = get_nodes(data)\n",
    "    edges = get_edges(data, nodes)\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(list(nodes.keys()))\n",
    "    graph.add_edges_from(edges)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:47.762419300Z",
     "start_time": "2024-01-20T22:20:46.443395300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27718\n"
     ]
    }
   ],
   "source": [
    "graph = form_graph(final_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2Vec\n",
    "\n",
    "Here the task is to perform random walks on the graph formed in the previous step and compute embeddings for the nodes using the random walk results.\n",
    "\n",
    "You can use gensim to compute embeddings, however for random walks you are expected to implement without relying on networkx. Your weblab assignment would aid you in the same. For gensim you are expected to use Word2Vec. However you  can explore on best ways to configure the hyperparams for your word2vec instance for better donwstream classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:47.775394700Z",
     "start_time": "2024-01-20T22:20:47.754393500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find isolated nodes with no neighbors before random walks\n",
    "### START\n",
    "isolated = []\n",
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:47.829417300Z",
     "start_time": "2024-01-20T22:20:47.771393600Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_walks(G: nx.Graph, num_walks: int, walk_length: int, isolated: List) -> np.ndarray:\n",
    "    \"\"\"Perform random walks on the graph.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): The graph.\n",
    "        num_walks (int): The number of random walks for each node.\n",
    "        walk_length (int): The number of nodes in a random walk.\n",
    "        isolated (List[int]): list of isolated nodeids\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The random walks, shape (n_nodes * num_walks, walk_length)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    ### START\n",
    "    \n",
    "    ## END\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:47.830394700Z",
     "start_time": "2024-01-20T22:20:47.787395400Z"
    }
   },
   "outputs": [],
   "source": [
    "walks = random_walks(graph, 8, 15,isolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:47.838414200Z",
     "start_time": "2024-01-20T22:20:47.802400500Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def fit_node2vec(walks: np.ndarray, vector_size: int, window: int, epochs: int) -> Word2Vec:\n",
    "    \"\"\"Train a Node2Vec model on random walks. Uses the GenSim Word2Vec implementation.\n",
    "\n",
    "    Args:\n",
    "        walks (np.ndarray): The random walks.\n",
    "        vector_size (int): Node representation size.\n",
    "        window (int): Window width.\n",
    "        epochs (int): Number of epochs.\n",
    "\n",
    "    Returns:\n",
    "        Word2Vec: The trained model.\n",
    "    \"\"\"\n",
    "   ### START\n",
    "\n",
    "   ### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:47.840418100Z",
     "start_time": "2024-01-20T22:20:47.817394200Z"
    }
   },
   "outputs": [],
   "source": [
    "model = fit_node2vec(walks, 128, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:47.856419100Z",
     "start_time": "2024-01-20T22:20:47.833393Z"
    }
   },
   "outputs": [],
   "source": [
    "#embeddings = {doc: model.dv[doc] for doc in model.dv.index_to_key}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:47.886417400Z",
     "start_time": "2024-01-20T22:20:47.853394800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nodeids(data: pd.DataFrame):\n",
    "    \"\"\"Get nodeids from graph\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): medium articles dataset\n",
    "\n",
    "    Returns:\n",
    "        nodes (dict): nodeids with mapped article titles\n",
    "    \"\"\"\n",
    "    nodes = {}\n",
    "    for index, row in data.iterrows():\n",
    "        if row[\"index\"] not in nodes:\n",
    "            nodes[row[\"index\"]] = row[\"title\"]\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:49.159392800Z",
     "start_time": "2024-01-20T22:20:47.865394200Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Get train and test nodes from the graph\n",
    "    \"\"\"\n",
    "train_nodes = get_nodeids(train)\n",
    "test_nodes = get_nodeids(test)\n",
    "train_nodes = list(train_nodes.keys())\n",
    "test_nodes = list(test_nodes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:49.187393300Z",
     "start_time": "2024-01-20T22:20:49.160392700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3950"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:49.296394200Z",
     "start_time": "2024-01-20T22:20:49.177393100Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"Get node2vec embeddings (nodeid to embeddings mapping)\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m {word: model\u001B[38;5;241m.\u001B[39mwv[word] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwv\u001B[49m\u001B[38;5;241m.\u001B[39mindex_to_key}\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "\"\"\"Get node2vec embeddings (nodeid to embeddings mapping)\n",
    "    \"\"\"\n",
    "embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec + Node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.289394Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Compute train and test embeddings.\n",
    "Concatenate the word2vec embeddings of article titles\n",
    "with the node2vec embeddings suing dictionary from previous step.\n",
    "Dimension 1 of your embeddings should be 256.\n",
    " Consider isolated nodes and handle them when computing embeddings\"\"\"\n",
    "X_train_n2v = [] ### STARt ### END\n",
    "X_test_n2v = [] ### START ### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.293394200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_n2v.shape,X_train_n2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.296394200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_n2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.299393600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train_n2v,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.302393500Z"
    }
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.307393700Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_n2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:20:49.318419800Z",
     "start_time": "2024-01-20T22:20:49.311394800Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = svc.predict(X_test_n2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.314395100Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.316420400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.f1_score(Y_test, predictions,average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.318419800Z"
    }
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only node2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.320393400Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_n2v =  np.array([embeddings[str(x)] if x not in isolated else np.zeros((128)) for idx, x in enumerate(train_nodes)  ], dtype=np.float32)\n",
    "\n",
    "X_test_n2v =np.array([embeddings[str(x)] if x not in isolated else np.zeros((128)) for idx, x in enumerate(test_nodes)  ], dtype=np.float32)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.322393700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train_n2v,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.326394900Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = svc.predict(X_test_n2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.328392500Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.f1_score(Y_test, predictions,average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-20T22:20:49.331394100Z"
    }
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, predictions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Compare performance of word2vec only, node2vec embeddings only (above) and word2vec+node2vec settings.\n",
    "  Report your intuition below in text as to why one works better than other and to what extent network structure helps compare dto only using word2vec embeddings of titles and subtitles (first result).\n",
    "\n",
    " You can also plot the tsne plot of embeddings to gain more intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
